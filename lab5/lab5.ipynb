{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3ce419-8d56-423c-9c63-61230db63427",
   "metadata": {},
   "source": [
    "# Lab 5: Practical Evaluation of Machine Learning Models\n",
    "\n",
    "In this lab, we're going to optimize the neural network trained for CIFAR-10 with quantization and compare the original and optimized models in terms of energy, latency, and memory.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- LO1: Learn how to measure the performance of ML model\n",
    "- LO2: Optimize the ML model for the CIFAR-10 dataset in order to improve the latency and reduce energy consumption\n",
    "- LO3: Project-Based Lab: Deep learning processing competition\n",
    "  - Select a dataset suitable for ARM Cortex-M processors\n",
    "  - Implement deep learning processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622858b4-19d7-46b0-97da-64cc939caca2",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113ecef-641c-4e44-b328-8e60a0159e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization, Activation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "import random, tempfile, zipfile\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760ef21-ad74-4c31-9f20-a9f7ce6bb169",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d33d9-f5e3-419d-aa95-ee84d2abc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from TF Keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# CIFAR10 class names\n",
    "class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fcdaf",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Here we normalise the images to be between 0 and 1 which is good practisse in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00619893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Print arrays shape\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e215dc-066b-4c38-a753-a63e16400ee2",
   "metadata": {},
   "source": [
    "### Previous model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0216f-3d54-4e70-bed2-270af43dec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keras model\n",
    "path_models = \"./Data/models/\"\n",
    "path_keras_model = path_models+\"custom_cifar10_model.h5\"\n",
    "\n",
    "model = tf.keras.models.load_model(path_keras_model)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5471c",
   "metadata": {},
   "source": [
    "# Quantisation\n",
    "In TinyML applications, one of the most crucial bottlenecks is the heavily constrained MCU memory. Thus quantisation is a very important asset in any TinyML engineering project. The gist of it is to shrink the full precision network while maintaining the models performance. Thus we quantise the 32 bit full precision network which we previously trained, down to an int8 quantised model. We do so, by first converting the keras model into a TFLite model (withoud quantisation) and then apply post-training quantisation utilising the build in converter of TFLite.\n",
    "\n",
    "Note the _representative dataset_ this essential for the converter to fight the right scaling of the model s.t. the quantisation has minimal impact on the models performance.\n",
    "\n",
    "\n",
    "_Theoretically any quantisation level is possible, yet int8 is the lowest supported level in TensorFlowLite._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2d42f",
   "metadata": {},
   "source": [
    "### TFLite Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6eaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert using integer-only quantization\n",
    "# Now you have an integer quantized model that uses integer data for \n",
    "# the model's input and output tensors, so it's compatible with integer-only hardware\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "path_models = \"./Data/models/\"\n",
    "path_keras_model = path_models+\"custom_cifar10_model.h5\"\n",
    "model = tf.keras.models.load_model(path_keras_model)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] #converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_model_quantInt = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577276a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tflite_model = path_models+\"custom_cifar10_model_quantInt.tflite\"\n",
    "\n",
    "# Save the quantized int model:\n",
    "with open(path_tflite_model, 'wb') as f:\n",
    "    f.write(tflite_model_quantInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5096db2",
   "metadata": {},
   "source": [
    "### Inspecting the quantised model\n",
    "Lets have a closer look at the quantised model. In this cell you can see that the input and output type of the model is in fact int8. Further you can see the quantisation scaling. This is important to scale the input and output back to the original data, as previously evaluated by the TFLite converter and the representative dataset.\n",
    "\n",
    "From the [TFlite Documentation](https://www.tensorflow.org/lite/performance/quantization_spec):\n",
    "\n",
    "_real_value = (int8_value - zero_point) * scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output details\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=path_tflite_model)\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"Name:\", input_details[0]['name'])\n",
    "print(\"Shape:\", input_details[0]['shape'])\n",
    "print(\"Type:\", input_details[0]['dtype'])\n",
    "print(\"quantisation scale {}, zero_point {}\".format(input_details[0]['quantization'][0], input_details[0]['quantization'][1]))\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"Name:\", output_details[0]['name'])\n",
    "print(\"Shape:\", output_details[0]['shape'])\n",
    "print(\"Type:\", output_details[0]['dtype'])\n",
    "print(\"quantisation scale {}, zero_point {}\".format(output_details[0]['quantization'][0], output_details[0]['quantization'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c31e9",
   "metadata": {},
   "source": [
    "### Benefit and drawback of the quantisation\n",
    "In Engineering there is no free lunch. While the benefit of quantisation is model size reduction, it comes at the cost of reduced model precision due to the int8 quantisation. The following cells will look at the model size reduction and the accuracy degredation. Now the TinyML engineer must decide if the tradeoff is worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89498e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "    # Returns size of gzipped model, in bytes.\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "fp_size = get_gzipped_model_size(path_keras_model)\n",
    "quant_size = get_gzipped_model_size(path_tflite_model)\n",
    "print('Size of Full Precision Model: {} Bytes'.format(fp_size))\n",
    "print('Size of quantised Model: {} Bytes'.format(quant_size))\n",
    "print('Size reduction factor: {} times'.format(fp_size/quant_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29808cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We evaluate on only a fraction of the test set for time reasons\n",
    "predictions = np.zeros((int(len(x_test)/100),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(int(len(x_test)/100)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == np.argmax(y_test[i])):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 100\n",
    "\n",
    "full_precision_model = tf.keras.models.load_model(path_keras_model)\n",
    "score = full_precision_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))\n",
    "print(\"Compared to float32 accuracy of {}%\".format(score[1]*100))\n",
    "print(\"We have a change of {}%\".format((accuracy_score-score[1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94feb1",
   "metadata": {},
   "source": [
    "## Code Credit\n",
    "Largely contributed\n",
    "- author = \"Pau Danilo Email: danilo.pau@st.com, Carra Alessandro\"\n",
    "- copyright = \"Copyright (c) 2018, STMicroelectronics\"\n",
    "- license = \"CC BY-NC-SA 3.0 IT - https://creativecommons.org/licenses/by-nc-sa/3.0/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
