{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: STM32 Cube AI and TensorFlow, Activity Recognition with Multi-Layer Perceptron (MLP)\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Train the first model with TensorFlow and deploy the inference on a microcontroller using STM32 Cube AI\n",
    "- Project-Based Lab: Develop the activity recognition application using multi-layer perceptron (MLP)\n",
    "- Evaluate with and without feature extraction. Run and evaluate the performance on a STM32 microcontroller\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import serial.tools.list_ports\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "samples_dir = os.path.join(base_dir, 'Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the MCU to the host PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Com ports list:')\n",
    "comPorts = list(serial.tools.list_ports.comports())\n",
    "for comPort in comPorts:\n",
    "    print(comPort)\n",
    "chooseComPort = input('Please insert port number: ')\n",
    "ser = serial.Serial('COM{}'.format(chooseComPort), 115200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(value):\n",
    "    value = value.replace(\"b' \", \"\")\n",
    "    vals = value.split(\", \")\n",
    "    del vals[-1]\n",
    "    results = list(map(int, vals))\n",
    "    return results\n",
    "\n",
    "def convert_list_to_df(lst):\n",
    "    x = lst[0::3]\n",
    "    y = lst[1::3]\n",
    "    z = lst[2::3]\n",
    "    df = pd.DataFrame({'X': x, 'Y': y, 'Z': z})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "Input the letter of the sample you want to acquire. The MCU will send the XYZ Accelerometer data as a timeseries of length 30. Run the cell -> Press the blue button on the MCU and move it until the green LED turns off -> Insert the number 1 into the prompt to acquire the data or number 2 to exit -> Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "letter = input('Please insert letter to collect data: ')\n",
    "stride = 30\n",
    "f = os.path.join(samples_dir, 'letter_{}_stride_{}.csv'.format(letter, stride))\n",
    "if os.path.exists(f):\n",
    "    print('File exists and data will be appended...')\n",
    "    xyz_df = pd.read_csv(f)\n",
    "else:\n",
    "    print('New sample, starting blank...')\n",
    "    xyz_df = pd.DataFrame(columns=['X', 'Y', 'Z'])\n",
    "\n",
    "while input('1 - acquire sample, 2 - exit: ') == '1':\n",
    "    line = ser.readline()\n",
    "    lineList = convert_to_list(str(line))\n",
    "    new_df = convert_list_to_df(lineList)\n",
    "    print('New data acquired:\\n', new_df.describe())\n",
    "    xyz_df = pd.concat([xyz_df, new_df], ignore_index=True)\n",
    "    print('Total Data count:', int(xyz_df.shape[0]/stride))\n",
    "\n",
    "print('Saving data to:', f)\n",
    "print('Total data of sample {}:\\n'.format(letter), xyz_df.describe())\n",
    "xyz_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all samples raw. Note that a single Accelerometer data batch will only contain 30 time steps. Thus to acquire single batches we must use an according \"stride\" of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [file for file in os.listdir(samples_dir) if '.csv' in file]\n",
    "\n",
    "stride = 30\n",
    "data = []\n",
    "labels = []\n",
    "for idx, file in enumerate(data_files):\n",
    "    df = pd.read_csv(os.path.join(samples_dir, file))\n",
    "    x = df['X'].to_numpy()\n",
    "    y = df['Y'].to_numpy()\n",
    "    z = df['Z'].to_numpy()\n",
    "    \n",
    "    for i in range(int(df.shape[0]/stride)):\n",
    "        base_idx = i * stride\n",
    "        batch = np.array([x[base_idx:base_idx+stride], y[base_idx:base_idx+stride], z[base_idx:base_idx+stride]])\n",
    "        batch = batch.reshape((3, stride))\n",
    "        data.append(batch)\n",
    "        labels.append(idx)\n",
    "        \n",
    "    print('Added {} data to the data list with label: {}'.format(file, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot one data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_sample(data_sample, label='Not Specified'):\n",
    "    plt.clf()\n",
    "    scaling = 2**10 #The STM ADC is 10bit so scale to get [g]\n",
    "    fig, axs = plt.subplots(3)\n",
    "    t = np.linspace(0, data_sample.shape[1] * 100, data_sample.shape[1]) #Accelerometer sampled with 100ms\n",
    "    axs[0].set_title(label='Single Data Sample of Label {}'.format(label))\n",
    "    axs[0].plot(t, data_sample[0]/scaling, c='m')\n",
    "    axs[0].set_ylabel('X [g]')\n",
    "    plt.setp(axs[0].get_xticklabels(), visible=False)\n",
    "    axs[1].plot(t, data_sample[1]/scaling, c='m')\n",
    "    axs[1].set_ylabel('Y [g]')\n",
    "    plt.setp(axs[1].get_xticklabels(), visible=False)\n",
    "    axs[2].plot(t, data_sample[2]/scaling, c='m')\n",
    "    axs[2].set_ylabel('Z [g]')\n",
    "    plt.xlabel('Time [ms]')\n",
    "    plt.show()\n",
    "    \n",
    "idx = random.randint(0, len(data)-1)\n",
    "plot_single_sample(data_sample=data[idx], label=labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and training\n",
    "Here, we create a neural network and train it! It is very small and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sklearn.utils.shuffle(np.array(data), np.array(labels))\n",
    "y_train = tf.keras.utils.to_categorical(y_train, len(np.unique(y_train)))\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(3, stride)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(len(np.unique(y_train)), activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=1, epochs=200, validation_split=0.4)\n",
    "\n",
    "model.save('raw_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1000])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of the Adam optimizer is 0.001. If the learning rate is too high, the model is more likely to overshoot the minia. On the other hand, if the learning rate is too small, the model reaches to the minia too slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High learning rate (lr = 1000)\n",
    "\n",
    "model_hlr = tf.keras.models.clone_model(model)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 1000)\n",
    "model_hlr.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_hlr.fit(x_train, y_train, batch_size=32, epochs=100, validation_split=0.4)\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1000])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low learning rate (lr = 0.0001)\n",
    "\n",
    "model_llr = tf.keras.models.clone_model(model)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.0001)\n",
    "model_llr.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_llr.fit(x_train, y_train, batch_size=32, epochs=100, validation_split=0.4)\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1000])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "Here we perform a single inference of the model. \n",
    "Run the cell -> Draw one of your letter classes with the MCU again (try to keep the movements similar as before) -> Press enter to acquire the MCU data -> view the model prediction. Did it do well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('Press Enter once MCU is ready')\n",
    "line = ser.readline()\n",
    "lineList = convert_to_list(str(line))\n",
    "new_df = convert_list_to_df(lineList)\n",
    "print('New data acquired:\\n', new_df.describe())\n",
    "\n",
    "x = new_df['X'].to_numpy()\n",
    "y = new_df['Y'].to_numpy()\n",
    "z = new_df['Z'].to_numpy()\n",
    "\n",
    "inf_data = np.array([x, y, z])\n",
    "plot_single_sample(data_sample=inf_data.reshape((3, stride)))\n",
    "#For inference we have to explicitly tell the model that the data has a batchsize of 1\n",
    "inf_data = inf_data.reshape((1, 3, stride))\n",
    "\n",
    "pred = model.predict(inf_data)\n",
    "print('Model Prediction: ', np.argmax(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "As the previously trained model performence is rather poor (because we don't have much data to train with). We will now be showing the powerfull effect of feature extraction.\n",
    "\n",
    "![Feature Extraction](feature.png)\n",
    "\n",
    "Instead of directly using the raw data to train a network, in some applications better results are achieved by first preprocessing the data into more abstract features, to _condense_ the contained information into fewer data points. This not only leads to less memory required for the features, but also to potentially smaller networks and better performance. Finding good features can be very hard, but for many applications you can read papers about which features perform well and how to calculate them.\n",
    "\n",
    "In this task a sliding window is used to calculate the piece wise mean and variance of the accelerator data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [file for file in os.listdir(samples_dir) if '.csv' in file]\n",
    "\n",
    "stride = 30\n",
    "slidingWindowExt = 6\n",
    "feature_data = []\n",
    "feature_labels = []\n",
    "for idx, file in enumerate(data_files):\n",
    "    df = pd.read_csv(os.path.join(samples_dir, file))\n",
    "    x = df['X'].to_numpy()\n",
    "    y = df['Y'].to_numpy()\n",
    "    z = df['Z'].to_numpy()\n",
    "    \n",
    "    for i in range(int(df.shape[0]/stride)):\n",
    "        base_idx = i * stride\n",
    "\n",
    "        # Mean feature\n",
    "        x_mean_ext = np.array([np.mean(x[i:i + slidingWindowExt]) for i in range(base_idx, base_idx + stride, slidingWindowExt)])\n",
    "        y_mean_ext = np.array([np.mean(y[i:i + slidingWindowExt]) for i in range(base_idx, base_idx + stride, slidingWindowExt)])\n",
    "        z_mean_ext = np.array([np.mean(z[i:i + slidingWindowExt]) for i in range(base_idx, base_idx + stride, slidingWindowExt)])\n",
    "        # STD feature\n",
    "        x_std_ext = np.array([np.std(x[i:i + slidingWindowExt]) for i in range(base_idx, base_idx + stride, slidingWindowExt)])\n",
    "        y_std_ext = np.array([np.std(y[i:i + slidingWindowExt]) for i in range(base_idx, base_idx + stride, slidingWindowExt)])\n",
    "        z_std_ext = np.array([np.std(z[i:i + slidingWindowExt]) for i in range(base_idx, base_idx + stride, slidingWindowExt)])\n",
    "        \n",
    "        batch = np.array([x_mean_ext, y_mean_ext, z_mean_ext, x_std_ext, y_std_ext, z_std_ext])\n",
    "        feature_data.append(batch)\n",
    "        feature_labels.append(idx)\n",
    "        \n",
    "    print('Added {} data to the feature data list with label: {}'.format(file, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_feature_sample(data_sample, label='Not Specified'):\n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(3)\n",
    "    t = np.linspace(0, data_sample.shape[1], data_sample.shape[1])\n",
    "    scaling = 2**10 #The STM ADC is 10bit so scale to get [g]\n",
    "    axs[0].set_title(label='Single Data Sample of Label {}'.format(feature_labels[idx]))\n",
    "    \n",
    "    xf_means = data_sample[0] / scaling\n",
    "    yf_means = data_sample[1] / scaling\n",
    "    zf_means = data_sample[2] / scaling\n",
    "    \n",
    "    xf_stds = data_sample[3] / scaling\n",
    "    yf_stds = data_sample[4] / scaling\n",
    "    zf_stds = data_sample[5] / scaling\n",
    "    \n",
    "    axs[0].grid()\n",
    "    axs[0].plot(t, xf_means, c='m')\n",
    "    axs[0].fill_between(t, xf_means - xf_stds, xf_means + xf_stds, alpha=0.2, color='m')\n",
    "    axs[0].set_ylabel('X [g]')\n",
    "    plt.setp(axs[0].get_xticklabels(), visible=False)\n",
    "    axs[1].grid()\n",
    "    axs[1].plot(t, yf_means, c='m')\n",
    "    axs[1].fill_between(t, yf_means - yf_stds, yf_means + yf_stds, alpha=0.2, color='m')\n",
    "    axs[1].set_ylabel('Y [g]')\n",
    "    plt.setp(axs[1].get_xticklabels(), visible=False)\n",
    "    axs[2].grid()\n",
    "    axs[2].plot(t, zf_means, c='m')\n",
    "    axs[2].fill_between(t, zf_means - zf_stds, zf_means + zf_stds, alpha=0.2, color='m')\n",
    "    axs[2].set_ylabel('Z [g]')\n",
    "    \n",
    "    plt.xlabel('Sample')\n",
    "    plt.show()\n",
    "    \n",
    "idx = random.randint(0, len(feature_data)-1)\n",
    "plot_single_feature_sample(data_sample=feature_data[idx], label=feature_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-based model\n",
    "The neural network basically uses the same architecture as before. Simply the input shape has changed. So we can show the effect that preprocessing your data has on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sklearn.utils.shuffle(np.array(feature_data), np.array(feature_labels))\n",
    "y_train = tf.keras.utils.to_categorical(y_train, len(np.unique(y_train)))\n",
    "\n",
    "data_shape = x_train[0].shape\n",
    "model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=data_shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(data_shape[0] * data_shape[1], activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(len(np.unique(y_train)), activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-based model inference\n",
    "Here we perform a single inference of the model, but with the feature based model! \n",
    "Run the cell -> Draw one of your letter classes with the MCU again (try to keep the movements similar as before) -> Press enter to acquire the MCU data -> view the model prediction. How does it perform now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('Press Enter once MCU is ready')\n",
    "line = ser.readline()\n",
    "lineList = convert_to_list(str(line))\n",
    "new_df = convert_list_to_df(lineList)\n",
    "print('New data acquired:\\n', new_df.describe())\n",
    "\n",
    "x = new_df['X'].to_numpy()\n",
    "y = new_df['Y'].to_numpy()\n",
    "z = new_df['Z'].to_numpy()\n",
    "\n",
    "# Mean feature\n",
    "x_mean_ext = np.array([np.mean(x[i:i + slidingWindowExt]) for i in range(0, stride, slidingWindowExt)])\n",
    "y_mean_ext = np.array([np.mean(y[i:i + slidingWindowExt]) for i in range(0, stride, slidingWindowExt)])\n",
    "z_mean_ext = np.array([np.mean(z[i:i + slidingWindowExt]) for i in range(0, stride, slidingWindowExt)])\n",
    "# STD feature\n",
    "x_std_ext = np.array([np.std(x[i:i + slidingWindowExt]) for i in range(0, stride, slidingWindowExt)])\n",
    "y_std_ext = np.array([np.std(y[i:i + slidingWindowExt]) for i in range(0, stride, slidingWindowExt)])\n",
    "z_std_ext = np.array([np.std(z[i:i + slidingWindowExt]) for i in range(0, stride, slidingWindowExt)])\n",
    "\n",
    "inf_data = np.array([x_mean_ext, y_mean_ext, z_mean_ext, x_std_ext, y_std_ext, z_std_ext])\n",
    "#For inference we have to explicitly tell the model that the data has a batchsize of 1\n",
    "plot_single_feature_sample(data_sample=inf_data)\n",
    "inf_data = inf_data.reshape((1, data_shape[0], data_shape[1]))\n",
    "\n",
    "pred = model.predict(inf_data)\n",
    "print('Model Prediction: ', np.argmax(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the feature model an some input data together with the respective output for STM Cube AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.npy', 'wb') as f:\n",
    "    np.save(f, x_train)\n",
    "\n",
    "with open('test_out.npy', 'wb') as f:\n",
    "    np.save(f, y_train)\n",
    "    \n",
    "model.save('feature_mlp.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
