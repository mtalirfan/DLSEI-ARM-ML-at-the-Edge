{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Speech Recognition with Mel-Frequency Cepstral Coefficient and Convolutional Neural Network\n",
    "\n",
    "In this exercise you will learn an efficient method for processing audio recordings using Mel-frequency cepstral coefficient (MFCC) and a convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC feature extraction and network training\n",
    "\n",
    "![audio_workflow](audio_workflow.png)\n",
    "\n",
    "The idea is to segment one slice of a continuous input signal into (overlapping)\n",
    "segments, do some preprocessing, and stack the resulting processed segments over one-another to\n",
    "obtain a two-dimensional frame. On these frames the same principles are used as for ”real”\n",
    "images when it comes to machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the \"Hey Snips\" dataset\n",
    "For this exercise we are going to use the ”Hey Snips” dataset from Couke et al. 2018 ”Efficient\n",
    "keyword spotting using dilated convolutions and gating”. The train dataset consists of utterances\n",
    "from 30 speakers, the test dataset consists of utterances from 10 speakers. The length of each\n",
    "utterance ranges between about 3 and 9 seconds, the data is labeled 1 if the sentence ”Hey Snips!”\n",
    "was uttered and 0 if not.\n",
    "\n",
    "Please visit the [Dataset Repository](https://github.com/sonos/keyword-spotting-research-datasets) and fill out the form to proceed with the dataset download. It is roughly 8.4GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import zipfile, tempfile\n",
    "import simpleaudio as sa\n",
    "\n",
    "DataSetPath = \"hey_snips_research_6k_en_train_eval_clean_ter/\"\n",
    "\n",
    "with open(DataSetPath+\"train.json\") as jsonfile:\n",
    "    traindata = json.load(jsonfile)\n",
    "\n",
    "with open(DataSetPath+\"test.json\") as jsonfile:\n",
    "    testdata = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data \n",
    "Here we load the data from the \"hey_snips\" dataset and extract the MFCC features from it. \n",
    "\n",
    "This is an example of how a data sample looks like.\n",
    "Information from the [Dataset Repository](https://github.com/sonos/keyword-spotting-research-datasets).\n",
    "\n",
    "```\n",
    "  {\n",
    "    \"id\": \"40084ea8-c576-4dba-a20b-fbda61f1de7d\"\n",
    "    \"is_hotword\": 1, \n",
    "    \"worker_id\": 12, \n",
    "    \"duration\": 1.86, \n",
    "    \"audio_file_path\": \"audio_files/40084ea8-c576-4dba-a20b-fbda61f1de7d.wav\", \n",
    "  }\n",
    "```\n",
    "- `id:` A unique identifier located in the audio_files subdirectory of the dataset\n",
    "- `is_hotword:` A 1 if the sample is a hotword and a 0 if not. This is what we are actually after in this classification model. Thus it acts as the label.\n",
    "- `worker_id:` The unique identifier of the contributor - note that worker ids are not consistent across datasets 1 and 2 as they are re-generated for each one of them.\n",
    "- `duration:` The duration of the audiofile in seconds.\n",
    "- `audio_file_path:` The filepath to the respective audio sample.\n",
    "\n",
    "But now we need to bring it into a format that we can actually learn in our machine learning model. So lets look at how that conversion looks like:\n",
    "\n",
    "\n",
    "\n",
    "1. We read the .wav file under the `audio_path`, this returns the sample rate (in samples/sec) and data from an LPCM WAV file. For more information see the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html).\n",
    "\n",
    "2. We zero-stuff the training and test samples so we have a constant slice length. As in the initial image.\n",
    "\n",
    "3. Now we segment the slices. For simplifying this exercise, we will choose the segment overlap to be 0. We might also want our segments to have a certain length - Powers of 2 will enable us to do faster FFTs.\n",
    "\n",
    "For time reasons we will also not load all training and test samples, but a smaller subset of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at a single example\n",
    "Here we perform the previously mentioned preprocessing steps. You will hear a random .wav sample of the test set and see how we aggregate the data and label.\n",
    "\n",
    "_Turn on your speakers!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = 100 # Number of loaded testing samples\n",
    "totalSliceLength = 10 # Length to stuff the signals to, given in seconds\n",
    "\n",
    "fs = 16000 # Sampling rate of the samples\n",
    "segmentLength = 1024 # Number of samples to use per segment\n",
    "\n",
    "sliceLength = int(totalSliceLength * fs / segmentLength)*segmentLength\n",
    "\n",
    "rand_idx = random.randrange(0, testsize)\n",
    "fs, test_sound_data = wavfile.read(DataSetPath+testdata[rand_idx]['audio_file_path'])\n",
    "_x_test = test_sound_data.copy()\n",
    "_x_test.resize(sliceLength)\n",
    "_x_test = _x_test.reshape((-1,int(segmentLength)))\n",
    "_x_test.astype(np.float32)\n",
    "label = testdata[rand_idx]['is_hotword']\n",
    "\n",
    "#Play the .wav file\n",
    "wave_obj = sa.WaveObject.from_wave_file(DataSetPath+testdata[rand_idx]['audio_file_path'])\n",
    "play_obj = wave_obj.play()\n",
    "play_obj.wait_done()\n",
    "\n",
    "print('Data: ', _x_test) #Notice the zero stuffing\n",
    "print('Label: ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap it in a helper function and perform it on a subspace of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    totalSliceLength = 10 # Length to stuff the signals to, given in seconds\n",
    "\n",
    "    trainsize = 1000 # Number of loaded training samples\n",
    "    testsize = 100 # Number of loaded testing samples\n",
    "\n",
    "\n",
    "    fs = 16000 # Sampling rate of the samples\n",
    "    segmentLength = 1024 # Number of samples to use per segment\n",
    "\n",
    "    sliceLength = int(totalSliceLength * fs / segmentLength)*segmentLength\n",
    "\n",
    "    for i in tqdm(range(trainsize)): \n",
    "        fs, train_sound_data = wavfile.read(DataSetPath+traindata[i]['audio_file_path']) # Read wavfile to extract amplitudes\n",
    "\n",
    "        _x_train = train_sound_data.copy() # Get a mutable copy of the wavfile\n",
    "        _x_train.resize(sliceLength) # Zero stuff the single to a length of sliceLength\n",
    "        _x_train = _x_train.reshape(-1,int(segmentLength)) # Split slice into Segments with 0 overlap\n",
    "        x_train_list.append(_x_train.astype(np.float32)) # Add segmented slice to training sample list, cast to float so librosa doesn't complain\n",
    "        y_train_list.append(traindata[i]['is_hotword']) # Read label \n",
    "\n",
    "    for i in tqdm(range(testsize)):\n",
    "        fs, test_sound_data = wavfile.read(DataSetPath+testdata[i]['audio_file_path'])\n",
    "        _x_test = test_sound_data.copy()\n",
    "        _x_test.resize(sliceLength)\n",
    "        _x_test = _x_test.reshape((-1,int(segmentLength)))\n",
    "        x_test_list.append(_x_test.astype(np.float32))\n",
    "        y_test_list.append(testdata[i]['is_hotword'])\n",
    "\n",
    "    x_train = tf.convert_to_tensor(np.asarray(x_train_list))\n",
    "    y_train = tf.keras.utils.to_categorical(np.asarray(y_train_list), num_classes=2)\n",
    "\n",
    "    x_test = tf.convert_to_tensor(np.asarray(x_test_list))\n",
    "    y_test = tf.keras.utils.to_categorical(np.asarray(y_test_list), num_classes=2)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC feature extraction\n",
    "The state-of-the art for the convolutional approach in speech recognition is structured around MFCC features, which is a non-linear mapping of the frequency spectrum to considerably fewer features. It is an effective feature extractor for audio classification.\n",
    "It converts conventional frequency to Mel scale, which is a logarithmic transformation that takes into account human perception at different frequencies.\n",
    "As MFCC has proven to be a very good feature for such audio tasks, there is an integrated function for it in TensorFlow, which we'll be using here. If you'd like more details, visit the [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/signal/linear_to_mel_weight_matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfccs(tensor):\n",
    "    sample_rate = 16000.0\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    frame_length = 1024\n",
    "    num_mfcc = 13\n",
    "\n",
    "    stfts = tf.signal.stft(tensor, frame_length=frame_length, frame_step=frame_length, fft_length=frame_length)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "    spectrograms = tf.reshape(spectrograms, (spectrograms.shape[0],spectrograms.shape[1],-1))\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "      num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
    "      upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfcc]\n",
    "    return tf.reshape(mfccs, (mfccs.shape[0],mfccs.shape[1],mfccs.shape[2],-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply load the data with the previously explained preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "x_train_mfcc = compute_mfccs(x_train)\n",
    "x_test_mfcc = compute_mfccs(x_test)\n",
    "\n",
    "print('Training Data Shape: ', x_train_mfcc.shape)\n",
    "print('Testing Data Shape: ', x_test_mfcc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an MFCC \"frame\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "mfcc_data=x_test_mfcc[rand_idx,:,:,0].numpy()\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(mfcc_data)\n",
    "ax.set_xlabel(\"MFCC Coefficients\")\n",
    "ax.set_ylabel(\"Time Segments\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the hyperparameters for the CNN training and apply a scaling to the MFCC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "train_set = (x_train_mfcc/512 + 0.5)\n",
    "train_labels = y_train\n",
    "\n",
    "test_set = (x_test_mfcc/512 + 0.5)\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN architecture\n",
    "We are using a CNN to architecture to fuse information over multiple channels efficiently. Such an architecture has proven to be very effective in such a problem setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "input_shape = (train_set.shape[1],train_set.shape[2],train_set.shape[3])\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "model.add(tf.keras.layers.Conv2D(filters=3,kernel_size=(3,3),padding=\"same\", activation='relu', input_shape=input_shape))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),strides=(2,2),padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(2,2),padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=48,kernel_size=(3,3),padding='same',strides=(2,2), activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compile the model by defining the loss functions, the optimizer, and the metrics to be used. We then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "model.fit(train_set, y_train, batchSize, epochs, validation_data=(test_set, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the trained model on the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(test_set, y_test, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the model and deploy it using CubeAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MFCCmodel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfd424c2c15cb0090681715105541f27309d5d99a849746e237f3250e06a7f9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
